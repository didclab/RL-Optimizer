{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ddad9c5-6be8-40a1-98ea-4a18130da3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjamil/venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:2: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils.version import LooseVersion\n"
     ]
    }
   ],
   "source": [
    "from a2c_agent import *\n",
    "from networks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9741dd-ca32-412c-8a75-edc7c43a2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45259604-e6f3-4c21-ba31-005ad3480385",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=A2cAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a8eee-c145-4dee-98ba-f22483a6e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.training_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daedf6d-d8ef-4e25-8a2f-03f7d6937190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(agent.scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('Training score of CartPole Actor-Critic TD(0)')\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(agent.scores)).reshape(-1, 1), np.array(agent.scores).reshape(-1, 1))\n",
    "y_pred = reg.predict(np.arange(len(agent.scores)).reshape(-1, 1))\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffddab-180f-4c9f-8da6-dcc43ecdb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411f52f-523a-4911-a360-bebbd2b519ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "state = agent.env.reset()\n",
    "scores = []\n",
    "import tqdm\n",
    "for _ in tqdm.notebook.tqdm(range(50)):\n",
    "    state = agent.env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action, lp = select_action(agent.actor_network, state,agent.DEVICE)\n",
    "        new_state, reward, done, info = agent.env.step(action)\n",
    "        score += reward\n",
    "        state = new_state\n",
    "    scores.append(score)\n",
    "agent.env.close()\n",
    "np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c9ce46-1a05-4b02-a869-3247981f51e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32, 34.9238114, 40, 10, 30), (32, 34.9238114, 40, 10, 40), (32, 34.9238114, 40, 10, 50), (32, 222.6392977, 40, 10, 30), (32, 222.6392977, 40, 10, 40), (32, 222.6392977, 40, 10, 50), (32, 222.6392977, 40, 10, 60), (32, 222.64, 40, 10, 30), (32, 222.64, 40, 10, 40), (32, 222.64, 40, 10, 50), (32, 222.64, 40, 10, 60), (32, 222.64, 40, 10, 70), (32, 222.64, 40, 10, 80), (32, 222.64, 40, 10, 90), (64, 222.6343047, 40, 10, 30), (64, 222.6343047, 40, 10, 40), (64, 222.6343047, 40, 10, 60), (128, 222.7841896, 40, 10, 30), (128, 222.7841896, 40, 10, 40), (250, 2.410113346, 40, 10, 30), (250, 2.410113346, 40, 10, 40), (250, 2.410113346, 40, 10, 50), (250, 2.410113346, 40, 10, 60), (250, 2.410113346, 40, 10, 70), (250, 2.410113346, 40, 10, 80), (250, 2.410113346, 40, 10, 90), (500, 0.098544573, 40, 10, 30), (500, 0.098544573, 40, 10, 40), (500, 100.91, 40, 10, 30), (500, 100.91, 40, 10, 40), (500, 100.91, 40, 10, 50), (500, 100.91, 40, 10, 70), (500, 100.91, 40, 10, 80), (500, 100.91, 40, 10, 90), (2500, 2.40051372, 40, 10, 30), (2500, 2.40051372, 40, 10, 40), (2500, 2.40051372, 40, 10, 50), (10000, 0.099238106, 40, 10, 30)]\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "from fileData import *\n",
    "from netEnv import *\n",
    "import time\n",
    "\n",
    "\n",
    "code_path = \"../../\"\n",
    "requiredFields = ['FileCount', 'AvgFileSize', 'BufSize', 'Bandwidth', 'AvgRtt', 'CC_Level', 'P_Level', 'PP_Level',\n",
    "                  'numActiveCores', 'frequency', 'TotalAvgTput', 'TotalEnergy', 'DataTransferEnergy']\n",
    "LabelName = 'TotalAvgTput'\n",
    "fileData_chameleon = ReadFile(code_path + '/Dataset/Chameleon_Combined_all.csv', requiredFields)\n",
    "filedata_grouped_df = fileData_chameleon.get_grouped_df()\n",
    "filedata_keys=[key for key, _ in filedata_grouped_df]\n",
    "print(filedata_keys)\n",
    "print(len(filedata_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e9992f-e3dc-46ad-bfb7-4b8f8162e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "36\n",
      "#################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjamil/venv/lib/python3.10/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/hjamil/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/hjamil/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/hjamil/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/hjamil/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/home/hjamil/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/home/hjamil/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "environmentG1=environmentGroups(fileData_chameleon.get_grouped_df(),fileData_chameleon.group_keys[0:1],0)\n",
    "env=NetEnvironment(environmentG1,fileData_chameleon.group_keys[0:1])\n",
    "\n",
    "print(env.observation_space.shape[0])\n",
    "print(env.action_space.n)\n",
    "print(\"#################\")\n",
    "env.reset()\n",
    "agent=A2cAgent(env,DISCOUNT_FACTOR=0.8, NUM_EPISODES=1000,MAX_STEPS=10000,SOLVED_SCORE=900)\n",
    "# t = time.time()\n",
    "# agent.training_agent(NUM_EPISODES=1000,MAX_STEPS=10000,SOLVED_SCORE=900)\n",
    "# print(f\"training up time is {time.time()- t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a133fd30-647f-4008-8b40-03a390600fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 1000 900 cuda\n"
     ]
    }
   ],
   "source": [
    "print(agent.MAX_STEPS, agent.NUM_EPISODES, agent.SOLVED_SCORE, agent.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98990d6-9566-4ef2-aaa4-c8957ca1d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm.notebook.tqdm(range(1)):\n",
    "    state = agent.env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    I = 1\n",
    "    for step in range(1000):\n",
    "          #get action and log probability\n",
    "        action, lp = select_action(agent.actor_network, state,agent.DEVICE)\n",
    "        #step with action\n",
    "        new_state, reward, done, _ = agent.env.step(action)\n",
    "        #update episode score\n",
    "        score += reward\n",
    "        #get state value of current state\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.DEVICE)\n",
    "        state_val = self.value_network (state_tensor)\n",
    "        #get state value of next state\n",
    "        new_state_tensor = torch.from_numpy(new_state).float().unsqueeze(0).to(self.DEVICE)\n",
    "        new_state_val = self.value_network(new_state_tensor)\n",
    "        #if terminal state, next state val is 0\n",
    "        if done:\n",
    "            new_state_val = torch.tensor([0]).float().unsqueeze(0).to(self.DEVICE)\n",
    "        #calculate value function loss with MSE\n",
    "        val_loss = F.mse_loss(reward + self.DISCOUNT_FACTOR * new_state_val, state_val)\n",
    "        val_loss *= I\n",
    "\n",
    "        #calculate policy loss\n",
    "        advantage = reward + self.DISCOUNT_FACTOR * new_state_val.item() - state_val.item()\n",
    "        policy_loss = -lp * advantage\n",
    "        policy_loss *= I\n",
    "        #Backpropagate policy\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        #Backpropagate value\n",
    "        self.stateval_optimizer.zero_grad()\n",
    "        val_loss.backward()\n",
    "        self.stateval_optimizer.step()\n",
    "        #move into new state, discount I\n",
    "        state = new_state\n",
    "        I *= self.DISCOUNT_FACTOR\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    #append episode score\n",
    "    print(f\"reward from episode {episode} is {score}\")\n",
    "    self.scores.append(score)\n",
    "    self.recent_scores.append(score)\n",
    "\n",
    "  #early stopping if we meet solved score goal\n",
    "    if np.array(self.recent_scores).mean() >= SOLVED_SCORE:\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
